{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-28T10:29:13.557287Z",
     "start_time": "2024-07-28T10:29:13.533287Z"
    }
   },
   "source": [
    "import functools\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, \\\n",
    "    Trainer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "wandb.init(mode=\"disabled\")\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:29:14.189116Z",
     "start_time": "2024-07-28T10:29:13.558290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_food = pd.read_parquet(\"../dataset/cisbox_food_gpt4.parquet\")\n",
    "df_beverage = pd.read_parquet(\"../dataset/cisbox_beverage_gpt4.parquet\")\n",
    "df_other = pd.read_parquet(\"../dataset/cisbox_other_gpt4.parquet\")"
   ],
   "id": "7df92455587c4a44",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:29:14.283115Z",
     "start_time": "2024-07-28T10:29:14.190116Z"
    }
   },
   "cell_type": "code",
   "source": "df_other = df_other.sample(n=len(df_food)).reset_index(drop=True)",
   "id": "36ded97adb3b0d69",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:29:14.314640Z",
     "start_time": "2024-07-28T10:29:14.285119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_beverage[\"labels\"] = 0\n",
    "df_food[\"labels\"] = 1\n",
    "df_other[\"labels\"] = 2\n",
    "\n",
    "df = pd.concat([df_food, df_beverage, df_other])"
   ],
   "id": "68e792e648d7916b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:29:14.516151Z",
     "start_time": "2024-07-28T10:29:14.315640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.dropna(inplace=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ],
   "id": "4eeeaa67337c563e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:29:14.562152Z",
     "start_time": "2024-07-28T10:29:14.517154Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(df[\"artikelbezeichnung1\"], df[\"labels\"], test_size=0.2, random_state=19)",
   "id": "20002b31b119d73b",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:29:20.407126Z",
     "start_time": "2024-07-28T10:29:20.124349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': X_train, 'labels': y_train}),\n",
    "    'val': Dataset.from_dict({'text': X_test, 'labels': y_test})\n",
    "})"
   ],
   "id": "67c1a8a2abefc0de",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:51:30.830677Z",
     "start_time": "2024-07-28T10:29:21.264158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model name\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "\n",
    "\n",
    "# preprocess dataset with tokenizer\n",
    "def tokenize_examples(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['text'])\n",
    "    tokenized_inputs['labels'] = examples['labels']\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\n",
    "tokenized_ds = tokenized_ds.with_format('torch')\n",
    "\n",
    "# qunatization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant=True,  # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # optimized fp format for ML\n",
    ")\n",
    "\n",
    "# lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # the dimension of the low-rank matrices\n",
    "    lora_alpha=8,  # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout=0.05,  # dropout probability of the LoRA layers\n",
    "    bias='none',  # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type='SEQ_CLS'\n",
    ")\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, quantization_config=quantization_config,\n",
    "                                                           num_labels=3)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ],
   "id": "96eb522cc19580d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e58eb8bcdd446ca82832bb157236f7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae27228a6c924cd896030c4df8a86630"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83f6eeae6e284c7d933c6f31d88f7440"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "952a65f4ad454ff7ab9c84d1e6e686c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/539110 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c613d0a775ad44b88e07a4f7e581c953"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/134778 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14e0dc43097a4363a6e9ed3a5f1cf434"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da402e1e66f14ba8a948f510b9f23060"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd8b64d3c9934afea950108dfbcd36d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3ee792cda36469198fba31da342470c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1a5af12ddd24c81897e17147efd148e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2db11dd8fbd3420cbbb3bdc033eab04a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15c228edc4ca470395a6b1310632ddac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:51:30.845678Z",
     "start_time": "2024-07-28T10:51:30.834679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define custom batch preprocessor\n",
    "def collate_fn(batch, tokenizer):\n",
    "    dict_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "    d = {k: [dic[k] for dic in batch] for k in dict_keys}\n",
    "    d['input_ids'] = torch.nn.utils.rnn.pad_sequence(d['input_ids'], batch_first=True,\n",
    "                                                     padding_value=tokenizer.pad_token_id)\n",
    "    d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(d['attention_mask'], batch_first=True, padding_value=0)\n",
    "    d['labels'] = torch.stack(d['labels'])\n",
    "    return d\n",
    "\n",
    "\n",
    "# define which metrics to compute for evaluation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1_weighted,\n",
    "    }"
   ],
   "id": "3b975c5e3eac3482",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:52:08.842831Z",
     "start_time": "2024-07-28T10:52:08.827833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = F.cross_entropy(logits, labels, reduction='mean')\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "id": "6722f7f4d4e87e8f",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:52:09.548596Z",
     "start_time": "2024-07-28T10:52:09.543596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 16\n",
    "epochs = 100\n",
    "weight_decay = 0.01"
   ],
   "id": "b51b4069ec047f49",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T10:52:10.275446Z",
     "start_time": "2024-07-28T10:52:10.076446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../outputs/llm/mistral',\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    metric_for_best_model='eval_f1',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=functools.partial(collate_fn, tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "id": "20ceb6ed135820d4",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-28T10:52:11.365141Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "625feccf2a4a2ab2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85423' max='3369500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  85423/3369500 41:58:24 < 1613:42:27, 0.57 it/s, Epoch 2.54/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.180215</td>\n",
       "      <td>0.946935</td>\n",
       "      <td>0.946840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.182787</td>\n",
       "      <td>0.947966</td>\n",
       "      <td>0.947863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "93832ae6a4aeed59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
