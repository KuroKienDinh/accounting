{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from utils.utils import extract_category_by_level, mapping_categories_training\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "import warnings\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('../data/group_cate_item_mapped_mainword.csv', encoding='utf-16le')\n",
    "df_standard_categories = pd.read_csv('../data/mindmap.csv', encoding='iso-8859-1')\n",
    "col_train = \"bez\"\n",
    "saved_name = \"bert_full_item\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_data.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_standard_categories.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df_standard_categories.columns:\n",
    "    df_standard_categories[col] = df_standard_categories[col].str.lower()\n",
    "df_data.dropna(subset=[col_train], inplace=True)\n",
    "df_data['category'] = df_data['category'].str.lower()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categories = extract_category_by_level(df_standard_categories, category='f&b', level=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_data['category_training'] = df_data['category'].apply(lambda x: mapping_categories_training(x, categories))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_data = df_data[df_data['category_training'] != 'other']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_data.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_data['category_training']\n",
    "X_text = df_data[col_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_Model(nn.Module):\n",
    "    def __init__(self, pretrained_text_model, n_classes):\n",
    "        super(Create_Model, self).__init__()\n",
    "        self.pretrained_text_model = pretrained_text_model\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_text = nn.Linear(768, 512)  # Linear layer after BERT\n",
    "        self.output = nn.Linear(512, n_classes)  # Final output layer\n",
    "\n",
    "    def forward(self, input_id, attention_mask):\n",
    "        # Assuming that 'input_id' and 'attention_mask' are provided as arguments\n",
    "        train_input = {'input_ids': input_id, 'attention_mask': attention_mask}\n",
    "\n",
    "        # Forward pass through the pre-trained model\n",
    "        embedding_text = self.pretrained_text_model(**train_input, return_dict=False)[1]\n",
    "\n",
    "        # Forward pass through additional layers\n",
    "        x = self.linear_text(embedding_text)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x  # Removed softmax here; it'll be applied in CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_path = '../model/bert_full_item_0.788.pt'\n",
    "\n",
    "with open(f'../model/{saved_name}_id2label.txt', 'r') as f:\n",
    "    id2label = eval(f.read())\n",
    "    \n",
    "label2id = {k: v for v, k in id2label.items()}\n",
    "y_labels = to_categorical([label2id[i] for i in y], dtype=\"uint8\")\n",
    "\n",
    "model = torch.load(checkpoint_path)\n",
    "max_acc = 0.788"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train from scratch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# id2label = {v: k for v, k in enumerate(y.unique())}\n",
    "# label2id = {k: v for v, k in enumerate(y.unique())}\n",
    "# y_labels = to_categorical([label2id[i] for i in y], dtype=\"uint8\")\n",
    "# \n",
    "# with open(f'../model/{saved_name}_id2label.txt', 'w') as f:\n",
    "#     f.write(str(id2label))\n",
    "# \n",
    "# pretrain_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "# \n",
    "# # for param in pretrain_model.parameters():\n",
    "# #     param.requires_grad = False\n",
    "# \n",
    "# # Freeze layers: Only leave the last 6 layers to be trainable\n",
    "# for param in list(pretrain_model.parameters())[:-6]:\n",
    "#     param.requires_grad = False\n",
    "# \n",
    "# model = Create_Model(pretrain_model, n_classes=len(label2id))\n",
    "# max_acc = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "length_train_idx = int(len(df_data) * 0.8)\n",
    "\n",
    "X_train = X_text\n",
    "X_test = X_text.loc[length_train_idx:]\n",
    "\n",
    "y_train = y_labels\n",
    "y_test = y_labels[length_train_idx:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_labels.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts_data, labels):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        self.labels = labels  #y_labels\n",
    "        self.texts = list(np.array(texts_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.labels[idx]\n",
    "        x_text = self.tokenizer(self.texts[idx], padding='max_length', max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "        return x_text, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, y_train, y_test, max_acc, learning_rate, batch_size, epochs,\n",
    "          device):\n",
    "    train, val = Dataset(X_train, y_train), Dataset(X_test, y_test)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    for epoch_num in range(epochs):\n",
    "        model.train()\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        total_batch_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_train += 1\n",
    "\n",
    "            # Move labels to the device and convert from one-hot to class indices\n",
    "            train_label = train_label.to(device)\n",
    "            train_label_indices = torch.argmax(train_label, dim=1)\n",
    "\n",
    "            attention_mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, attention_mask)\n",
    "\n",
    "            # Using class indices in CrossEntropyLoss\n",
    "            batch_loss = criterion(output, train_label_indices)\n",
    "            total_loss_train += batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # output = torch.nn.functional.softmax(output, dim=-1)\n",
    "            acc = ((output.argmax(-1) == train_label.argmax(-1)).sum()) / len(train_label)\n",
    "            total_acc_train += acc.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        # model.eval()\n",
    "        # total_acc_val = 0\n",
    "        # total_loss_val = 0\n",
    "        # total_batch_val = 0\n",
    "        # for val_input, val_label in tqdm(val_dataloader):\n",
    "        #     with torch.no_grad():\n",
    "        #         total_batch_val += 1\n",
    "        # \n",
    "        #         # Move labels to the device and convert from one-hot to class indices\n",
    "        #         val_label = val_label.to(device)\n",
    "        #         val_label_indices = torch.argmax(val_label, dim=1)\n",
    "        # \n",
    "        #         attention_mask = val_input['attention_mask'].to(device)\n",
    "        #         input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "        # \n",
    "        #         output = model(input_id, attention_mask)\n",
    "        # \n",
    "        #         # Using class indices in CrossEntropyLoss\n",
    "        #         batch_loss = criterion(output, val_label_indices)\n",
    "        #         total_loss_val += batch_loss.item()\n",
    "        # \n",
    "        #         # output = torch.nn.functional.softmax(output, dim=-1)\n",
    "        # \n",
    "        #         acc = ((output.argmax(-1) == val_label.argmax(-1)).sum()) / len(val_label)\n",
    "        #         total_acc_val += acc.item()\n",
    "        # \n",
    "        # if total_acc_val / (total_batch_val) > max_acc:\n",
    "        #     torch.save(model, f'../model/bert_full_item_model_{total_acc_val / (total_batch_val):.3f}.pt')\n",
    "        #     max_acc = total_acc_val / total_batch_val\n",
    "        # \n",
    "        # print(\n",
    "        #     f'Epochs:{epoch_num + 1} | Train Loss:{total_loss_train / (total_batch_train):.3f} | Train Accuracy:{total_acc_train / (total_batch_train):.3f} | Val Loss:{total_loss_val / (total_batch_val):.3f} | Val Accuracy:{total_acc_val / (total_batch_val):.3f}')\n",
    "        if total_acc_train / (total_batch_train) > max_acc:\n",
    "            torch.save(model, f'../model/{saved_name}_{total_acc_train / (total_batch_train):.3f}.pt')\n",
    "            max_acc = total_acc_train / total_batch_train\n",
    "\n",
    "        print(\n",
    "            f'Epochs:{epoch_num + 1} | Train Loss:{total_loss_train / (total_batch_train):.3f} | Train Accuracy:{total_acc_train / (total_batch_train):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LR = 0.0001\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "train(model, X_train, X_test, y_train, y_test, max_acc, LR, BATCH_SIZE, EPOCHS, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('../data/group_cate_item_mapped_mainword.csv', encoding='utf-16le')\n",
    "df_standard_categories = pd.read_csv('../data/mindmap.csv', encoding='iso-8859-1')\n",
    "\n",
    "for col in df_standard_categories.columns:\n",
    "    df_standard_categories[col] = df_standard_categories[col].str.lower()\n",
    "df_data.dropna(subset=[col_train], inplace=True)\n",
    "df_data['category'] = df_data['category'].str.lower()\n",
    "categories = extract_category_by_level(df_standard_categories, category='f&b', level=2)\n",
    "df_data['category_training'] = df_data['category'].apply(lambda x: mapping_categories_training(x, categories))\n",
    "df_data = df_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "with open(f'../model/{saved_name}_id2label.txt', 'r') as f:\n",
    "    id2label = eval(f.read())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Create_Model(nn.Module):\n",
    "    def __init__(self, pretrained_text_model, n_classes):\n",
    "        super(Create_Model, self).__init__()\n",
    "        self.pretrained_text_model = pretrained_text_model\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_text = nn.Linear(768, 512)  # Linear layer after BERT\n",
    "        self.output = nn.Linear(512, n_classes)  # Final output layer\n",
    "\n",
    "    def forward(self, input_id, attention_mask):\n",
    "        # Assuming that 'input_id' and 'attention_mask' are provided as arguments\n",
    "        train_input = {'input_ids': input_id, 'attention_mask': attention_mask}\n",
    "\n",
    "        # Forward pass through the pre-trained model\n",
    "        embedding_text = self.pretrained_text_model(**train_input, return_dict=False)[1]\n",
    "\n",
    "        # Forward pass through additional layers\n",
    "        x = self.linear_text(embedding_text)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x  # Removed softmax here; it'll be applied in CrossEntropyLoss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model = torch.load('../model/bert_full_item_model_0.870.pt')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_data.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bez_test = \"\"\"abcde 12n 12kg\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "test_token = tokenizer([bez_test], padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "input_id = test_token['input_ids'].squeeze(1).to(device)\n",
    "attention_mask = test_token['attention_mask'].to(device)\n",
    "output = model(input_id, attention_mask=attention_mask)\n",
    "output_proba = torch.nn.functional.softmax(output, dim=-1)\n",
    "output = output_proba.argmax(-1)[0].cpu().numpy().tolist()\n",
    "\"\"\"print output_proba with class name\"\"\"\n",
    "proba_format = {id2label[i]: proba for i, proba in enumerate(output_proba[0].cpu().detach().numpy().tolist())}\n",
    "print(proba_format)\n",
    "print(\"Class:\", id2label[output])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T02:38:51.797877300Z",
     "start_time": "2023-09-22T02:38:50.469425300Z"
    }
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, X_text_test, X_num_test, y_test, batch_size, device):\n",
    "#     val = Dataset(X_text_test, X_num_test, y_test)\n",
    "#     val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "#     model = model.to(device)\n",
    "#     model.eval()\n",
    "#     prediction = []\n",
    "#     for val_input, val_label in tqdm(val_dataloader):\n",
    "#         with torch.no_grad():\n",
    "#             val_label = val_label.to(device)\n",
    "#             attention_mask = val_input['attention_mask'].to(device)\n",
    "#             input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "#             output = model(input_id, attention_mask=attention_mask)\n",
    "#             output = torch.nn.functional.softmax(output, dim=-1)\n",
    "#             output = output.argmax(-1)[0].cpu().numpy().tolist()\n",
    "#             prediction.append(output)\n",
    "#     return prediction\n",
    "# \n",
    "# prediction = evaluate(model, X_test, None, y_test, batch_size=1, device=device)\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test.argmax(-1), prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
